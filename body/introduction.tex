\section{Introduction}
	%\begin{comment}
	%awesome introduction.
	%\begin{itemize}
	%	\item[Task]{Unsupervised word alignment.}
	%	\item[Research Questions]{Word embeddings include noisy vectors for word alignment.}
	%	\item[Approach]{Learn importance weights for each vectors to mitigate the noise.}
	%\end{itemize}
	%\end{comment}

	Multi-lingual word alignment is important to transferring knowledge between the languages by the carrier of word, which is an application of domain adaption. Based on the fundamental concept in cognitive language that the physiological basis of human beings plays an important role in formation of concepts and language (Add reference here), even the disparate origin of the languages has led to barrier of comprehension, same cognitive mechanism determine the feasibility to transfer the knowledge without supervision. For example, with the cognitive concept that the capital is a center of a entity in country category, " Tomboy " in French should have a high similarity with " London " in English. 

	There are several attempts on applying word distribution model in multi-lingual alignment model, which we call it multi-lingual distribution model. Word embedding has obtained huge success in wide range NLP downstream tasks like sentiment classification and reading comprehension e. T C (Add reference here) On the other side, multi-lingual distribution model aims at solving multi-lingual tasks ranging from as machine translation to cross-lingual search engine, which try to get several monolingual embeddings and their correspondences. Multi-lingual distribution model has been proved to be useful in cross-lingual transfer learning (Mikolov et al, 2013). 

	Mikolov et al. (2013a) found that linear relationship exists on two words embedding spaces by an empirical research, and show learning the linear mapping with a small starting dictionary can overmatch the neural network. As a further work, orthogonal transformations have been shown effective because its ability to keep distance invariance after transformation (Xing et al, 2015). 

	Nevertheless, in realistic conditions, there still exist some language pairs which don not have too much parallel data for learning. To figure out the problem, semi-supervised and unsupervised methods are developed recently. They have obtained obvious result even better than supervised methods. We focus on full-unsupervised method of this research. 

	Word embeddings suffer from drawback of uneven quality cause by different frequency of words. In unsupervised settings, because the word embeddings are the only resource we can use, this problem tends to be a significant factor influencing the accuracy of alignment. To settle this problem, (Conneau et, 2017) only use mutual nearest words pair through training to generate orthogonal mapping. On the other side, (David et, 2018) explicitly sets weight to decay rate by frequency as hyper parameters to each language to abate the influence of low frequency words. 

	However, all of these methods only consider the deficiency in monolingual but pay no attention to the noise in bilingual alignment. There are two main difficulties in bilingual alignment, the first originated from different semantic of identical entity by cultural difference. For example, different countries may have distinctive attitudes to an entity like color and person. Secondly, high frequency words not always have exact correspondence with / between the other language such as auxiliary word and symbol which have less information for alignment. 

	In this research, to settle this problem, we use Sinkhorn algorithm (Sinkhorn, 1964) to accustom weights of all words which can reduce transformation loss between two embedding sets. Our method achieves remarkable result and can observably improve the robustness of model to different experiment settings.

	The contributions to this paper are as follows :

	1. We give a systematic introduction to Multilingual distribution models. 
	2. We present a novel model by modifying the weight to loss of each word automatically and achieve better result than previous State-of-art works.  



